%use thsi function to generate a text file saving the boosting model. This
%model can be read by our C++ gentle boost routines, so we can use the
%classifiers in our code

%check website http://www.mathworks.com/help/stats/fitensemble.html to see
%the matlab class

%check
%C:\Users\Fernando\cppProjects\TrackingGaussianMixtures\trunk\backgroundDetection\MatlabCode
%to see examples on how to fit classifiers
function parseMatlabFitEnsembleToCppGentleBoostFormat(ens, fileout)

%disp 'TODO: consider ens.Trained{1}.Cost;!!!!!!!!!!!!!!!!!!!!!!!'

if( length(ens.ClassNames) ~= 2 && ens.ClassNames(2) ~= 1 )
    error ' Code not ready for non-binary classification methods where second class is +1'
end

Nweak = length(ens.Trained);
wTotal = sum( ens.TrainedWeights );

class2Idx = containers.Map(ens.PredictorNames,[1:length(ens.PredictorNames)]);%to map predictor names to variables

fid = fopen(fileout,'w');
fprintf(fid,'%d\n',Nweak);

%test on RUSboost ensemble: classreg.learning.classif.CompactClassificationTree
for kk = 1:Nweak
    %info on each element of ens.Trained{kk}
    %http://www.mathworks.com/help/stats/compactclassificationtreeclass.html
    
    
    switch( ens.Method )
       
        case 'GentleBoost'
            learner = ens.Trained{kk}.CompactRegressionLearner;
        case 'RUSBoost'
            learner = ens.Trained{kk};
        otherwise
            error 'Code not tested yet for this method'
    end
    
    %numnodes
     Nnodes = learner.NumNodes;
    fprintf(fid,'%d\n',Nnodes);
    
    %C++ format is 
    %out<<classifier[ii][jj].a <<" "<<classifier[ii][jj].b <<" "<<classifier[ii][jj].featureNdx <<" "<<classifier[ii][jj].gtChild <<" "<<classifier[ii][jj].leChild <<" "<<classifier[ii][jj].parentIdx <<" "<< classifier[ii][jj].th <<endl;
    
    %parse featureNdx
    qq = learner.CutVar;
    featureNdx = -ones(Nnodes,1);
    for ii = 1:length(qq)
        if( isempty ( qq{ii} ) == true )
            continue;
        end
        featureNdx(ii) = class2Idx(qq{ii}) -1; %Again 1-indexing versus zero indexing
    end
    
    %parse classiciation prediction for a and b parameters: f_m = a * (x_k > th) + b
    a = zeros(Nnodes,1);
    b = a;
    
    
    switch( ens.Method )
       
        case 'GentleBoost'
            disp 'WARNING: FLIPPING CLASS REGRESSIONS. MAKE SURE THIS IS CORRECT'            
            classProb = -learner.NodeMean;%regressions value between [-1,1]
        case 'RUSBoost'
            classProb = learner.ClassProb(:,2);%probability of y=+1     
        otherwise
            error 'Code not tested yet for this method'
    end
    
    w = ens.TrainedWeights(kk) / wTotal; % the classifier should output the weighted probability of belonging to class 1
    par = learner.Parent;
    for ii = 1: length(a)
        ch = learner.Children(ii,:);%ch(1) is the one for x_k<th -> b = w * probClass( ch(1),2); %second class is y = +1        
        if( ch(1) > 0 )
            b(ii) = w * classProb( ch(1));            
        end
        %a + b = w * probClass( ch(2),2);
        if( ch(2) > 0 )
           a(ii) =  w * classProb( ch(2) ) - b(ii);
        end
        
        if( ch(1) <= 0 && ch(2)<=0 )%reproduce class probabilities from leaf            
            b(ii) = w * classProb( ii); 
            a(ii) =  w * classProb( ii ) - b(ii);
            featureNdx(ii) = featureNdx( par(ii) );
        end
    end
    %{
    [label score] = predict (ensemble);
    score(ensemble): A matrix with one row per observation and one column per class. 
    For each observation and each class, the score generated by each tree is the probability
    of this observation originating from this class computed as the fraction of observations 
    of this class in a tree leaf. predict averages these scores over all trees in the ensemble.
    
    %}
    
    thr = learner.CutPoint;
    thr( isnan(thr) ) = 1e32;%so b is always chosen as the value
    %{
    switch( ens.Method )
       
        case 'GentleBoost'
            thr( isnan(thr) ) = thr( par( isnan(thr) ) );%regressions value between [-1,1]
        case 'RUSBoost'
            thr( isnan(thr) ) = 1e45;%so b is always chosen as the value
        otherwise
            error 'Code not tested yet for this method'
    end
    %}
    
    tree = [a,b,featureNdx,learner.Children(:,[2 1])-1, learner.Parent-1, thr ];%Matlab indexing is 1 while C++ is 0
    
    fprintf(fid,'%f %f %d %d %d %d %g\n',tree');       
end

